{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Scraper for the WallStreetBets subreddit created using PRAW and PSAW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This scraper produces JSON files of submissions from the WallStreetBets subreddit. Each JSON file contains a list of dictionaries with relevant information about the submission for my analysis: post title, text, label, upvotes, upvote ratio, id and URL. I initially used this to scrape 200k WallStreetBets posts \n",
    "\n",
    "I had to combine PRAW and PSAW because some of the specific reddit submission search fields that I needed (link_flair_text, score) were not accurate in PSAW, but PRAW's time-range search functionality has been disabled. This combined scraper first performs a time-range search on the AITA subreddit using PSAW. The submission ids from those results are then put intp PRAW, which mines the relevant submission information.  \n",
    "\n",
    "PRAW (https://praw.readthedocs.io/en/latest/) and PSAW were both used to create this scraper. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import praw\n",
    "reddit = praw.Reddit(client_id='SECRET', client_secret='SECRET', user_agent='SECRET')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from psaw import PushshiftAPI\n",
    "api = PushshiftAPI()\n",
    "subreddit = reddit.subreddit('wallstreetbets')\n",
    "\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_more_posts(start = datetime.date(2021,2,15), lim=1000):\n",
    "    \n",
    "    #initialize post list\n",
    "    posts = []        \n",
    "    \n",
    "    #search for posts from before the specified time in psaw\n",
    "    results = list(api.search_submissions(before=start,\n",
    "                                subreddit='wallstreetbets', #change subreddit\n",
    "                                filter=['url','num_comments','created_utc','id'], #change traits returned\n",
    "                                limit=lim))      \n",
    "    \n",
    "    for i in results:\n",
    "\n",
    "        try:\n",
    "            #insert the id of the results into PRAW\n",
    "            j = praw.models.Submission(reddit,id=i.id)\n",
    "\n",
    "            post_dict = {}\n",
    "            post_dict[\"title\"] = j.title\n",
    "            post_dict[\"text\"] = j.selftext\n",
    "            post_dict[\"label\"] = j.link_flair_text\n",
    "            post_dict['score'] = j.score\n",
    "            post_dict['ups'] = j.ups\n",
    "            post_dict['downs'] = j.downs\n",
    "            post_dict['upvote_ratio'] = j.upvote_ratio\n",
    "            post_dict['id'] = j.id\n",
    "            post_dict['url'] = j.url\n",
    "            post_dict['comments_id'] = [str(i) for i in j.comments]\n",
    "            post_dict['comments_text'] = []\n",
    "            post_dict[\"comment_scores\"] = []\n",
    "            \n",
    "            for i in j.comments:\n",
    "                try:\n",
    "                    post_dict[\"comments_text\"].append(i.body)\n",
    "\n",
    "                except:\n",
    "                    post_dict[\"comments_text\"].append(None)\n",
    "\n",
    "                try:\n",
    "                    post_dict[\"comment_scores\"].append(i.score)\n",
    "                except:\n",
    "                    post_dict[\"comment_scores\"].append(None)\n",
    "\n",
    "            post_dict['date'] = j.created_utc\n",
    "            post_dict['num_comments'] = len(post_dict['comments_id'])\n",
    "            posts.append(post_dict)\n",
    "            \n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    #return list of posts and the timestamp of the last post in the search. you use this to iterate further back in time\n",
    "    return posts, results[-1].created_utc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### test one search\n",
    "results = list(api.search_submissions(before=datetime.date(2020,1,14),\n",
    "                                subreddit='rateme', #change subreddit\n",
    "                                filter=['url','num_comments','created_utc','id','comments','body'], #change traits returned\n",
    "                                limit=1))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([{'title': 'Created a cliff for myself to jump off of',\n",
       "   'text': '[deleted]',\n",
       "   'label': 'Loss',\n",
       "   'score': 1,\n",
       "   'ups': 1,\n",
       "   'downs': 0,\n",
       "   'upvote_ratio': 1.0,\n",
       "   'id': 'lk0rzi',\n",
       "   'url': 'https://i.redd.it/aq6r2ogz7jh61.jpg',\n",
       "   'comments_id': ['gnhbg9b'],\n",
       "   'comments_text': [\"Screenshots of your positions must show equity or gains/losses of more than $2,500 or $10,000 USD for options or stocks respectively. \\n\\nWe don't just want a % change chart. Tell us what you traded, when, for how much, and why!\\n\\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/wallstreetbets) if you have any questions or concerns.*\"],\n",
       "   'comments_scores': [1],\n",
       "   'date': 1613347186.0,\n",
       "   'num_comments': 1},\n",
       "  {'title': 'I will ship this to the biggest loss porn from last week. Letâ€™s see some loss porn baby!!!',\n",
       "   'text': '',\n",
       "   'label': 'Loss',\n",
       "   'score': 1,\n",
       "   'ups': 1,\n",
       "   'downs': 0,\n",
       "   'upvote_ratio': 1.0,\n",
       "   'id': 'lk0rhj',\n",
       "   'url': 'https://i.redd.it/mq9zincu7jh61.jpg',\n",
       "   'comments_id': [],\n",
       "   'comments_text': [],\n",
       "   'comments_scores': [],\n",
       "   'date': 1613347139.0,\n",
       "   'num_comments': 0},\n",
       "  {'title': 'IPLY creators of Earth Worm Jim to the moon ðŸš€ðŸš€ðŸš€ðŸš€ love this game as a kid I want to see the studio return.',\n",
       "   'text': '',\n",
       "   'label': 'Meme',\n",
       "   'score': 1,\n",
       "   'ups': 1,\n",
       "   'downs': 0,\n",
       "   'upvote_ratio': 1.0,\n",
       "   'id': 'lk0rh4',\n",
       "   'url': 'https://i.redd.it/h865w9eu7jh61.jpg',\n",
       "   'comments_id': [],\n",
       "   'comments_text': [],\n",
       "   'comments_scores': [],\n",
       "   'date': 1613347138.0,\n",
       "   'num_comments': 0},\n",
       "  {'title': \"friends?! I'm not qualified to even post on wsb lol\",\n",
       "   'text': '[deleted]',\n",
       "   'label': None,\n",
       "   'score': 1,\n",
       "   'ups': 1,\n",
       "   'downs': 0,\n",
       "   'upvote_ratio': 1.0,\n",
       "   'id': 'lk0r7q',\n",
       "   'url': 'https://i.imgur.com/jlGcRnj.jpg',\n",
       "   'comments_id': [],\n",
       "   'comments_text': [],\n",
       "   'comments_scores': [],\n",
       "   'date': 1613347112.0,\n",
       "   'num_comments': 0},\n",
       "  {'title': 'Just added another 100$ to my Webull acc what we getting gains from next?',\n",
       "   'text': '[removed]',\n",
       "   'label': 'Gain',\n",
       "   'score': 1,\n",
       "   'ups': 1,\n",
       "   'downs': 0,\n",
       "   'upvote_ratio': 1.0,\n",
       "   'id': 'lk0r2h',\n",
       "   'url': 'https://www.reddit.com/r/wallstreetbets/comments/lk0r2h/just_added_another_100_to_my_webull_acc_what_we/',\n",
       "   'comments_id': ['gnhbam4'],\n",
       "   'comments_text': ['Hey! Your post has been removed because it breaks [rule 2](https://old.reddit.com/r/wallstreetbets/about/rules). It seems like it should have been a comment in the daily thread. We call this \"commentposting\".\\n\\nThis happens when your submission isn\\'t all that useful, funny, long, or unique, and is primarily text.\\n\\nNote: Discussion threads can steer clear of this by bringing some information to the table, being more than a ticker and a question mark, etc.\\n\\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/wallstreetbets) if you have any questions or concerns.*'],\n",
       "   'comments_scores': [1],\n",
       "   'date': 1613347098.0,\n",
       "   'num_comments': 1}],\n",
       " 1613347098)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### test function once \n",
    "\n",
    "get_more_posts(datetime.date(2021,2,15),lim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1611781244\n",
      "1611781028\n",
      "1611780820\n",
      "1611780624\n",
      "1611780435\n",
      "1611780256\n",
      "1611780085\n",
      "1611779901\n",
      "1611779729\n",
      "1611779513\n",
      "1611779320\n",
      "1611779129\n",
      "1611778950\n",
      "1611778781\n",
      "1611778602\n",
      "1611778464\n",
      "1611778291\n",
      "1611778151\n",
      "1611777986\n",
      "1611777834\n",
      "1611777691\n",
      "1611777567\n",
      "1611777422\n",
      "1611777264\n",
      "1611777100\n",
      "1611776939\n",
      "1611776775\n",
      "1611776595\n",
      "1611776419\n",
      "1611776182\n",
      "1611775931\n",
      "1611775612\n",
      "1611775396\n",
      "1611775164\n",
      "1611774984\n",
      "1611774794\n",
      "1611774614\n",
      "1611774376\n",
      "1611774085\n",
      "1611773902\n",
      "1611773757\n",
      "1611773610\n",
      "1611773423\n",
      "1611773231\n",
      "1611772995\n",
      "1611772889\n",
      "1611772797\n",
      "1611772692\n",
      "1611772595\n",
      "1611772486\n",
      "1611772402\n",
      "1611772309\n",
      "1611772205\n",
      "1611772103\n",
      "1611772021\n",
      "1611771924\n",
      "1611771824\n",
      "1611771720\n",
      "1611771616\n",
      "1611770894\n",
      "1611770800\n",
      "1611770712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\psaw\\PushshiftAPI.py:153: UserWarning: Unable to connect to pushshift.io. Retrying after backoff.\n",
      "  warnings.warn(\"Unable to connect to pushshift.io. Retrying after backoff.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1611770631\n",
      "1611770531\n",
      "1611770452\n",
      "1611770359\n",
      "1611770269\n",
      "1611770181\n",
      "1611770089\n",
      "1611770005\n",
      "1611769904\n",
      "1611769809\n",
      "1611769709\n",
      "1611769625\n",
      "1611769535\n",
      "1611769447\n",
      "1611769373\n",
      "1611769293\n",
      "1611769207\n",
      "1611769120\n",
      "1611769037\n",
      "1611768957\n",
      "1611768888\n",
      "1611768813\n",
      "1611768736\n",
      "1611768661\n",
      "1611768581\n",
      "1611768509\n",
      "1611768428\n",
      "1611768346\n",
      "1611768260\n",
      "1611768176\n",
      "1611768090\n",
      "1611768013\n",
      "1611767937\n",
      "1611767870\n",
      "1611767791\n",
      "1611767714\n",
      "1611767645\n",
      "1611767574\n",
      "1611767495\n",
      "1611767421\n",
      "1611767345\n",
      "1611767265\n",
      "1611767175\n",
      "1611767098\n",
      "1611767029\n",
      "1611766940\n",
      "1611766866\n",
      "1611766797\n",
      "1611766713\n",
      "1611766636\n",
      "1611766564\n",
      "1611766482\n",
      "1611766400\n",
      "1611766324\n",
      "1611766233\n",
      "1611766152\n",
      "1611766077\n",
      "1611766013\n",
      "1611765929\n",
      "1611765831\n",
      "1611765736\n",
      "1611765646\n",
      "1611765567\n",
      "1611765480\n",
      "1611765375\n",
      "1611765290\n",
      "1611765208\n",
      "1611765115\n",
      "1611765030\n",
      "1611764947\n",
      "1611764853\n",
      "1611764751\n",
      "1611764658\n",
      "1611764564\n",
      "1611764476\n",
      "1611764391\n",
      "1611764297\n",
      "1611764215\n",
      "1611764120\n",
      "1611764024\n",
      "1611763931\n",
      "1611763825\n",
      "1611763736\n",
      "1611763636\n",
      "1611763519\n",
      "1611763404\n",
      "1611763301\n",
      "1611763204\n",
      "1611763120\n",
      "1611763017\n",
      "1611762915\n",
      "1611762811\n",
      "1611762698\n",
      "1611762603\n",
      "1611762494\n",
      "1611762385\n",
      "1611762288\n",
      "1611762175\n",
      "1611762060\n",
      "1611761946\n",
      "1611761832\n",
      "1611761709\n",
      "1611761603\n",
      "1611761495\n",
      "1611761384\n",
      "1611761285\n",
      "1611761173\n",
      "1611761059\n",
      "1611760954\n",
      "1611760836\n",
      "1611760702\n",
      "1611760597\n",
      "1611760499\n",
      "1611760407\n",
      "1611760296\n",
      "1611760181\n",
      "1611760061\n",
      "1611759937\n",
      "1611759818\n",
      "1611759690\n",
      "1611759587\n",
      "1611759473\n"
     ]
    }
   ],
   "source": [
    "target = 10000000 #the number of posts you want to acquire\n",
    "post_list = [] #list of posts\n",
    "\n",
    "time = 1611781244 ## starting timed\n",
    "\n",
    "#an arbitrarily large number for the range so it doesn't stop before it needs to\n",
    "for i in range(10000000000000000000000000000):\n",
    "    \n",
    "    if len(post_list)<target: #continue using the get_more_posts function until post_list is long enough\n",
    "        \n",
    "        print(time) #optional for seeing when a new loop starts (and in case you need the time to iterate back futher)\n",
    "        \n",
    "        ### get 1000 posts (the max number needed)\n",
    "        (posts,time) = get_more_posts(time,1000)\n",
    "        post_list.extend(posts) \n",
    "        \n",
    "        ### save files from one iteration (these are smaller and easier to work with)\n",
    "        with open('wsb_psaw_praw_{}.json'.format(str(len(post_list))), 'w') as json_file: #optional, save files from one iteration\n",
    "            json.dump(post_list, json_file) \n",
    "    \n",
    "    ### once you are over the target number of posts\n",
    "    elif len(post_list)>target:\n",
    "        \n",
    "        \n",
    "        with open('wsb_psaw_praw_final_{}.json'.format(str(len(post_list))), 'w') as json_file: #save final file\n",
    "            json.dump(post_list, json_file)\n",
    "            \n",
    "        with open(\"final_time.json\",'w') as json_file: #save final timestamp in case you need to run some more\n",
    "            json.dump(time,json_file)\n",
    "            \n",
    "        print(time) #optional for seeing the last timestamp\n",
    "        \n",
    "        break #exit the loop\n",
    "        \n",
    "    else: # in case you can't hit the target for whatever reason\n",
    "        \n",
    "        with open('wsb_psaw_praw_final_{}.json'.format(str(len(post_list))), 'w') as json_file: #save final file\n",
    "            json.dump(post_list, json_file)\n",
    "            \n",
    "        with open(\"final_time.json\",'w') as json_file: #save final timestamp in case you need to run some more\n",
    "            json.dump(time,json_file)\n",
    "            \n",
    "        print(time) #optional for seeing the last timestamp\n",
    "        \n",
    "        break #exit the loop\n",
    "        \n",
    "    if time < 1611723584: ## temporary for extracting one day\n",
    "        break\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
